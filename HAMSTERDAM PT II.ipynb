{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global Variables\n",
    "timeout = 500 * 1000\n",
    "delta_threshold = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def density_xor(d):\n",
    "    reward = lambda x, y: x ^ y if (x + y) % 10 < d else np.random.uniform(-1, 0)\n",
    "    return reward\n",
    "\n",
    "class Grid: \n",
    "    def __init__(self, edge_len, start = (0,0), actions = lambda x: [ 'l', 'd', 'u', 'r' ], reward_func = density_xor(5) ):\n",
    "        self.edge = edge_len\n",
    "        self.actions = actions\n",
    "        self.start = start\n",
    "        self.reward_func = reward_func\n",
    "        \n",
    "    def transition(self, state, action):\n",
    "        x, y = state\n",
    "\n",
    "        if action == \"u\":\n",
    "            state = ( (x + 1) % self.edge, y )\n",
    "        elif action == \"d\":\n",
    "            state = ( (x - 1) % self.edge, y )\n",
    "        elif action == \"l\":\n",
    "            state = ( x, (y + 1) % self.edge )\n",
    "        elif action == \"r\":\n",
    "            state = ( x, (y - 1) % self.edge )\n",
    "\n",
    "        return self.reward_func(* state), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "        if x == 0:\n",
    "            reward = 1\n",
    "        elif x == 14 and (y == 13 or y == 14):\n",
    "            reward = 100\n",
    "        else:\n",
    "            reward = 1 / (abs(x - 14) + abs(y - 13)) / 5\n",
    "        \n",
    "        # class MarkovGrid(Grid):\n",
    "    def __init__(self, edge_len, pi, actions = lambda x: [ 0, 1 ]):\n",
    "        super().__init__(edge_len)\n",
    "        self.markov_state = 0\n",
    "        self.pi = pi\n",
    "        self.actions = actions\n",
    "        \n",
    "    def transition(self, state, action):\n",
    "        reward = (self.markov_state ^ action) * np.random.uniform(-2, 2)\n",
    "        self.markov_state = 0 if np.random.random() < self.pi[self.markov_state, 0] else 1\n",
    "        return reward, state\n",
    "\n",
    "    def initialize_q(self, grid):\n",
    "        self.q_vals = {}\n",
    "        for state in [ (i, j) for i in range(grid.edge) for j in range(grid.edge) ]:\n",
    "            for action in grid.actions(state):\n",
    "                self.q_vals[( state, action )] = 0.0\n",
    "        return grid\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Editorial Note: low rho make slow\n",
    "def sigmoid(x, rho = 1):\n",
    "    return (np.exp(rho * x) / (1 + np.exp(rho * x)))\n",
    "\n",
    "plt.plot( range(-11, 11), [ sigmoid(i) for i in range(-11, 11) ] )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class agent: \n",
    "    def __init__( self, alpha = 0.1, epsilon = 0.5, discount = 0.1, rhos = (1, 1), lags = -1 ) -> None:\n",
    "        self.q_vals, self.v_vals = defaultdict(lambda: 0), defaultdict(lambda: 0)\n",
    "        self.alpha, self.epsilon, self.discount = alpha, epsilon, discount\n",
    "        self.rho1, self.rho2 = rhos\n",
    "\n",
    "        self.TDES, self.L = [0], lags\n",
    " \n",
    "    def update(self, state, next_state, action, reward) -> float: \n",
    "        max_qval = max([ self.q_vals[(next_state, a)] for a in self.grid.actions(next_state) ])\n",
    "        previous = self.q_vals[(state, action)]\n",
    "        \n",
    "        self.q_vals[(state, action)] = (1 - self.alpha) * previous + self.alpha * (reward + self.discount * (max_qval))\n",
    "        self.v_vals[state] = self.v_vals[state] + 1\n",
    "        \n",
    "        delta = abs( self.q_vals[(state, action)] - previous )\n",
    "        self.TDES.append(delta)\n",
    "        \n",
    "        if self.L > -1 and len(self.TDES) > self.L:\n",
    "            self.TDES.pop(0)\n",
    "        return delta\n",
    "        \n",
    "    def epsilon_greedy(self, state) -> str:\n",
    "        max_action = max(self.grid.actions(state), key = lambda x: self.q_vals[(state, x)])\n",
    "        return max_action if np.random.random() < self.epsilon else np.random.choice( self.grid.actions(state) )\n",
    "        \n",
    "    def softmax(self, state, beta = 1) -> str:\n",
    "        q_vals = np.array( [ self.q_vals[(state, a)] for a in self.grid.actions(state) ])\n",
    "        softmax = np.exp((q_vals * beta)) / (np.sum(np.exp(q_vals * beta)))\n",
    "        return np.random.choice( self.grid.actions(state), p = softmax )\n",
    "    \n",
    "    def boltzmann(self, state) -> str:\n",
    "        return self.softmax(state, beta = self.epsilon)\n",
    "    \n",
    "    def mixture(self, state, epsilon = None) -> str:\n",
    "        epsilon = epsilon or self.epsilon\n",
    "        return self.softmax(state) if np.random.random() < epsilon else np.random.choice( self.grid.actions(state) )\n",
    "    \n",
    "    def phi_explore(self, state):\n",
    "        state_len = len(self.q_vals) + 1\n",
    "        state_sum = np.sum([ self.v_vals[i] for i in self.v_vals ]) + 1\n",
    "        return sigmoid((1 / state_len) - ( self.v_vals[state] / state_sum ), self.rho1)\n",
    "    \n",
    "    def phi_exploit(self, state):\n",
    "        relative = self.TDES[-1] / np.sum(self.TDES)\n",
    "        return sigmoid(1 - (relative * len(self.TDES)), self.rho2)\n",
    "\n",
    "    def inhomogeniety(self, state):\n",
    "        epsilon = self.phi_exploit(state)\n",
    "        return epsilon / (epsilon + self.phi_explore(state))\n",
    "        \n",
    "    def inhomogenous_greedy(self, state) -> str:\n",
    "        max_action = max(self.grid.actions(state), key = lambda x: self.q_vals[(state, x)])\n",
    "        return max_action if np.random.random() < self.inhomogeniety(state) else np.random.choice( self.grid.actions(state) )\n",
    "        \n",
    "    def inhomogenous_boltzmann(self, state) -> str:\n",
    "        return self.softmax(state, beta = self.inhomogeniety(state))\n",
    "    \n",
    "    def inhomogenous_mixture(self, state) -> str:\n",
    "        return self.softmax(state) if np.random.random() < self.inhomogeniety(state) else np.random.choice( self.grid.actions(state) )\n",
    "        \n",
    "    def play(self, grid, action_selection, limit):  \n",
    "        converge, i, state = 0, 0, grid.start\n",
    "        self.grid = grid\n",
    "        \n",
    "        while (converge < limit) and (i < timeout):\n",
    "            action = action_selection(self, state)\n",
    "            reward, next_state = self.grid.transition(state, action)\n",
    "\n",
    "            delta = self.update(state, next_state, action, reward)\n",
    "            converge = converge + 1\n",
    "            if delta > delta_threshold:\n",
    "                converge = 0\n",
    "            \n",
    "            state, i = next_state, i + 1\n",
    "            self.alpha = self.alpha * 0.9995\n",
    "        \n",
    "        average_reward, self.epsilon = 0, 0.9\n",
    "        for p in range(limit // 2):\n",
    "            action = self.epsilon_greedy(state)\n",
    "            reward, state = self.grid.transition(state, action)     \n",
    "            average_reward = average_reward + reward\n",
    "        \n",
    "        return i, (2 / limit) * average_reward\n",
    "    \n",
    "    def reset(self):\n",
    "        self.q_vals, self.v_vals = defaultdict(lambda: 0), defaultdict(lambda: 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = Grid(50) # MarkovGrid( 1, np.array([0.5, 0.5, 0.5, 0.5]).reshape((2, 2)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_agent = agent()\n",
    "_agent.play( grid, agent.inhomogenous_greedy, 100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def select_parameters( params ):\n",
    "    params.sort(key = lambda x: np.random.random() )\n",
    "    return params[: len(params) // 10 ]\n",
    "\n",
    "alpha_discount = select_parameters([ (i / 10, j / 10) for i in range(1, 10) for j in range(1, 10) ])\n",
    "rhos = select_parameters([ (i / 10, j / 10) for i in range(1, 200, 10) for j in range(1, 200, 10) ])\n",
    "\n",
    "size_limit = select_parameters([ (i, 10**j) for i in range(5, 50) for j in range(1, 5) ])\n",
    "\n",
    "def test_agent( action, descriptor = 'agent' ):\n",
    "    gc.collect()\n",
    "    endogenous, exogenous = [], []\n",
    "\n",
    "    for r1, r2 in tqdm(rhos):\n",
    "        for L in range(-1, 50, 10):\n",
    "            for alpha, discount in alpha_discount:\n",
    "                _agent = agent(alpha = alpha, discount = discount, rhos = (r1, r2), lags = L)\n",
    "\n",
    "                exog, endo = [], []\n",
    "                for size, limit in size_limit:\n",
    "                    for density in range(1, 10, 2):\n",
    "                        _grid = Grid(size, reward_func = density_xor(density))\n",
    "\n",
    "                        iterations, rewards = _agent.play( _grid, action, limit )\n",
    "\n",
    "                        endo.append([ rewards, iterations ])\n",
    "                        exog.append([ alpha, discount, r1, r2, L, size, limit ])\n",
    "\n",
    "                endo.sort( key = lambda x: np.random.random() )\n",
    "                exog.sort( key = lambda x: np.random.random() )\n",
    "\n",
    "                exogenous.extend( exog[: len(exog) // 10 ] )\n",
    "                endogenous.extend( endo[: len(endo) // 10 ] )\n",
    "\n",
    "    endogenous = np.array( endogenous, dtype = np.float32 )\n",
    "    exogenous  = np.array( exogenous,  dtype = np.float32 )\n",
    "\n",
    "    endo_filename = descriptor + '_endo.npy'\n",
    "    exog_filename = descriptor + '_exog.npy'\n",
    "    np.save(endo_filename, endogenous)\n",
    "    np.save(exog_filename, exogenous)\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/home/ameet/anaconda3/envs/int_dim/lib/python3.7/site-packages/ipykernel_launcher.py:47: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/home/ameet/anaconda3/envs/int_dim/lib/python3.7/site-packages/ipykernel_launcher.py:47: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/home/ameet/anaconda3/envs/int_dim/lib/python3.7/site-packages/ipykernel_launcher.py:47: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]\n",
      "100%|██████████| 40/40 [58:42<00:00, 88.06s/it]t]\n",
      " 32%|███▎      | 13/40 [1:24:23<2:37:35, 350.19s/it]/home/ameet/anaconda3/envs/int_dim/lib/python3.7/site-packages/ipykernel_launcher.py:47: RuntimeWarning: invalid value encountered in double_scalars\n",
      " 38%|███▊      | 15/40 [1:35:42<2:23:33, 344.54s/it]/home/ameet/anaconda3/envs/int_dim/lib/python3.7/site-packages/ipykernel_launcher.py:47: RuntimeWarning: invalid value encountered in double_scalars\n",
      "100%|██████████| 40/40 [3:57:13<00:00, 355.83s/it]it]\n",
      "100%|██████████| 40/40 [3:57:28<00:00, 356.22s/it]\n",
      " 15%|█▌        | 6/40 [10:32:04<59:17:40, 6278.25s/it]"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "actions = [(agent.boltzmann, 'boltzmann'), \\\n",
    "           (agent.epsilon_greedy, 'egreedy'), \\\n",
    "           (agent.inhomogenous_boltzmann, 'i_boltzmann'), \\\n",
    "           (agent.inhomogenous_greedy, 'i_greedy'), \\\n",
    "           (agent.inhomogenous_mixture, 'i_mixture'), \\\n",
    "           (agent.mixture, 'mixture'), ]\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "def run_test(x):\n",
    "    return test_agent(* x)\n",
    "\n",
    "pool = Pool()\n",
    "pool.map( run_test, actions, chunksize = 1 )\n",
    "pool.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.api import OLS\n",
    "\n",
    "def effect_estimation( descriptor ):\n",
    "    endo_filename = descriptor + '_endo.npy'\n",
    "    exog_filename = descriptor + '_exog.npy'\n",
    "    \n",
    "    endogenous = np.load(endo_filename)\n",
    "    exogenous  = np.save(exog_filename)\n",
    "    \n",
    "    return OLS(exogenous, endogenous).fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, filename in actions:\n",
    "    print( effect_estimator( filename ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def B(d1, d2):\n",
    "    assert len(d1) == len(d2)\n",
    "    return sum([ np.sqrt(d1[i] * d2[i]) for i in range(len(d1)) ])\n",
    "\n",
    "def KL(d1, d2):\n",
    "    assert len(d1) == len(d2)\n",
    "    return sum([ d1[i] * np.log( d1[i] / d2[i] ) for i in range(len(d1)) ])\n",
    "\n",
    "def normalize(d):\n",
    "    d = d + abs(min(d))\n",
    "    return [ i / sum(d) for i in d ]\n",
    "\n",
    "def RMSE_q_values(agent, state):\n",
    "    q_values = [ agent.q_vals[( state, a)] for a in agent.grid.actions(state) ]\n",
    "    \n",
    "    next_state = [ agent.grid.transition(state, a)[1] for a in agent.grid.actions(state) ]\n",
    "    true_rwrds = [ ((x * y) + (x - 25)) / 25 for x, y in next_state ]\n",
    "    \n",
    "    return np.linalg.norm(np.array(q_values) - np.array(true_rwrds))\n",
    "\n",
    "def RMSE_total(agent):\n",
    "    return np.sqrt(np.mean([ RMSE_q_values(agent, (i, j)) for i in range(agent.grid.edge) for j in range(agent.grid.edge) ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boltzma_list = []\n",
    "egreedy_list = []\n",
    "mixture_list = []\n",
    "\n",
    "for i in tqdm(range(1, 20)):\n",
    "    converge_limit = int(3 ** (i / 2)) \n",
    "    mix, greed, boltz = agent(grid), agent(grid), agent(grid)\n",
    "    \n",
    "    mixture_list.append( ( *mix.play( (0,1), mix.inhomogenous_mixture, True ), RMSE_total(mix)) )\n",
    "    egreedy_list.append( ( *greed.play( (0,1), greed.epsilon_greedy, True), RMSE_total(greed)) )\n",
    "    boltzma_list.append( ( *boltz.play( (0,1), boltz.boltzmann, True), RMSE_total(boltz)) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [ 3**(x / 2) for x in range(1, 20) ]\n",
    "\n",
    "plt.plot(X, [i[1] for i in egreedy_list], \"r\", X, [i[1] for i in boltzma_list], \"b\", X, [i[1] for i in mixture_list], \"g\")\n",
    "plt.title(\"Average Reward\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(X, [i[0] for i in egreedy_list], \"r\", X, [i[0] for i in boltzma_list], \"b\", X, [i[0] for i in mixture_list], \"g\")\n",
    "plt.title(\"Convergence (in Rounds)\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(X, [i[2] for i in egreedy_list], \"r\", X, [i[2] for i in boltzma_list], \"b\", X, [i[2] for i in mixture_list], \"g\")\n",
    "plt.title(\"RMSE of Q-Values from True Expectation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boltzmann_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, Y, Z, T = [], [], [], []\n",
    "\n",
    "N = 1000\n",
    "f = B #KL\n",
    "\n",
    "for epsilon in tqdm(range(1, 11)):\n",
    "    beta = epsilon / 10\n",
    "    a, b, c = 0, 0, 0\n",
    "    \n",
    "    for i in range(N):\n",
    "        rand_vec = np.random.random( 25 * 25 )\n",
    "    \n",
    "        boltz = np.exp(rand_vec * beta)/np.sum(np.exp(rand_vec * beta))\n",
    "        mixy  = (1 - beta) *  np.exp(rand_vec)/np.sum(np.exp(rand_vec)) + (beta) * 1 / (rand_vec.shape[0])\n",
    "        greed = (1 - beta) * (rand_vec == max(rand_vec)).astype(\"int\")  + (beta) * 1 / (rand_vec.shape[0])\n",
    "        \n",
    "        a = a + f(boltz, mixy)\n",
    "        b = b + f(boltz, greed)\n",
    "        c = c + f(mixy, greed)\n",
    "\n",
    "    X += [(10 - epsilon) / 10]\n",
    "    Y += [a / N]\n",
    "    Z += [b / N]\n",
    "    T += [c / N]\n",
    "\n",
    "plt.plot(X, Y, \"g\")\n",
    "plt.plot(X, Z, \"r\")\n",
    "plt.plot(X, T, \"b\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
